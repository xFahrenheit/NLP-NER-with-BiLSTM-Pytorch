{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import necessary libraries \nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom pandas.core.common import random_state","metadata":{"id":"OyBE8i-Cb-3_","execution":{"iopub.status.busy":"2023-03-28T04:15:41.244450Z","iopub.execute_input":"2023-03-28T04:15:41.245151Z","iopub.status.idle":"2023-03-28T04:15:41.250389Z","shell.execute_reply.started":"2023-03-28T04:15:41.245112Z","shell.execute_reply":"2023-03-28T04:15:41.249002Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# read the text file into a list of strings\nwith open('/data/train', 'r') as f:\n    data = f.read().splitlines()\n\n# split each string into three columns and store in a list\nrows = []\nfor line in data:\n    cols = line.split()\n    if len(cols) == 3:\n        rows.append(cols)\n\n# create a pandas dataframe from the list of rows\ndf = pd.DataFrame(rows, columns = [\"index\", \"token\", \"tag\"])\ndf['index'] = df['index'].astype(int)","metadata":{"id":"cb3p73ZfiyDG","execution":{"iopub.status.busy":"2023-03-28T04:15:41.256807Z","iopub.execute_input":"2023-03-28T04:15:41.257081Z","iopub.status.idle":"2023-03-28T04:15:41.637149Z","shell.execute_reply.started":"2023-03-28T04:15:41.257055Z","shell.execute_reply":"2023-03-28T04:15:41.636108Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-03-28T04:15:41.639127Z","iopub.execute_input":"2023-03-28T04:15:41.639507Z","iopub.status.idle":"2023-03-28T04:15:41.653090Z","shell.execute_reply.started":"2023-03-28T04:15:41.639452Z","shell.execute_reply":"2023-03-28T04:15:41.651994Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"        index       token     tag\n0           1          EU   B-ORG\n1           2     rejects       O\n2           3      German  B-MISC\n3           4        call       O\n4           5          to       O\n...       ...         ...     ...\n204562      1     Swansea   B-ORG\n204563      2           1       O\n204564      3     Lincoln   B-ORG\n204565      4           2       O\n204566      1  -DOCSTART-       O\n\n[204567 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>token</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>EU</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>rejects</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>German</td>\n      <td>B-MISC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>call</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>to</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>204562</th>\n      <td>1</td>\n      <td>Swansea</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>204563</th>\n      <td>2</td>\n      <td>1</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>204564</th>\n      <td>3</td>\n      <td>Lincoln</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>204565</th>\n      <td>4</td>\n      <td>2</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>204566</th>\n      <td>1</td>\n      <td>-DOCSTART-</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>204567 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# create mappings from words and NER tags to indices\nner2idx = {'O': 0, 'B-MISC':1, 'I-MISC':2, 'I-PER':3, 'B-LOC':4, 'I-ORG':5, 'B-PER':6, 'I-LOC':7, 'B-ORG': 8}\nword2idx = {'<PAD>': 0, 'unk': 1}\n\nidx = 2  # start indexing from 2\n\nfor word in df['token']:\n    if word not in word2idx:\n        word2idx[word] = idx\n        idx += 1\n\nword_list = list(word2idx.keys())","metadata":{"id":"n3nxlpXXXeeg","execution":{"iopub.status.busy":"2023-03-28T04:15:41.654933Z","iopub.execute_input":"2023-03-28T04:15:41.655688Z","iopub.status.idle":"2023-03-28T04:15:41.716916Z","shell.execute_reply.started":"2023-03-28T04:15:41.655627Z","shell.execute_reply":"2023-03-28T04:15:41.716012Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# load the glove embeddings\nglove_embeddings = {}\nwith open('glove.6B.100d', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        glove_embeddings[word] = embedding","metadata":{"execution":{"iopub.status.busy":"2023-03-28T04:15:41.719817Z","iopub.execute_input":"2023-03-28T04:15:41.720187Z","iopub.status.idle":"2023-03-28T04:15:49.746513Z","shell.execute_reply.started":"2023-03-28T04:15:41.720149Z","shell.execute_reply":"2023-03-28T04:15:49.745522Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"word_embed_matrix = []\n\n#create a random array of size 100 to assign it to unknown words\nrandom_array = np.concatenate((np.random.uniform(-1, 0, size=50), np.random.uniform(0, 1, size=50))) * np.random.choice([-1, 1], size=100)\n\ndef is_capitalized(word):\n    return word[0].istitle()\n\nfor word in word_list:\n    # check if the word is capitalized or not\n    if is_capitalized(word):\n        # if the word is capitalized, convert it to lower case and check if it exists in the glove embeddings\n        if word.lower() in glove_embeddings:\n            # if the lowercased word exists in the glove embeddings, get its embedding and concatenate a 1 to it\n            word_embedding = np.concatenate([glove_embeddings[word.lower()], np.asarray([1])], axis=0)\n        else:\n            # if the lowercased word does not exist in the glove embeddings, assign a random array of size 100 to assign it to unknown words\n            word_embedding = np.concatenate([random_array, np.asarray([1])], axis=0)\n    else:\n        # if the word is not capitalized, get its lowercased embedding from the glove embeddings (if it exists there) and concatenate a 0 to it\n        if word.lower() in glove_embeddings:\n            word_embedding = np.concatenate([glove_embeddings[word.lower()], np.asarray([0])], axis=0)\n        else:\n            word_embedding = np.concatenate([random_array, np.asarray([0])], axis=0) #random_array\n\n    word_embed_matrix.append(word_embedding)","metadata":{"id":"VnpXhK1W93Co","execution":{"iopub.status.busy":"2023-03-28T04:15:49.747989Z","iopub.execute_input":"2023-03-28T04:15:49.748379Z","iopub.status.idle":"2023-03-28T04:15:49.867816Z","shell.execute_reply.started":"2023-03-28T04:15:49.748341Z","shell.execute_reply":"2023-03-28T04:15:49.866883Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# initialize lists to store the encoded sentences and NER tags\nencoded_sentences = []\nencoded_ner_tags = []\nsentence = []\nner_tags = []\n\n# encode the sentences and NER tags into numerical representations\nfor i, row in df.iterrows():\n    if row['index'] == 1:  # start of a new sentence\n        if i > 0:\n            # append the encoded sentence and NER tags to the corresponding lists\n            encoded_sentences.append(sentence)\n            encoded_ner_tags.append(ner_tags)\n        # re-initialize the sentence and NER tags\n        sentence = []\n        ner_tags = []\n    # encode the current word and NER tag\n    sentence.append(word2idx.get(row['token'], word2idx['unk']))\n    ner_tags.append(ner2idx[row['tag']])\n\n# append the last encoded sentence and NER tags to the corresponding lists\nencoded_sentences.append(sentence)\nencoded_ner_tags.append(ner_tags)\n\n# pad the sequences to have the same length using PyTorch's pad_sequence function\nx = nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in encoded_sentences], batch_first=True, padding_value=0)\ny = nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in encoded_ner_tags], batch_first=True, padding_value=-1)\n\n# print the shape of the tensors\nprint('Encoded Sentences Shape:', x.shape)\nprint('Encoded NER Tags Shape:', y.shape)","metadata":{"id":"xWa2wyD793FS","outputId":"58cf1a29-5343-41ac-d23b-f03e60145e6a","execution":{"iopub.status.busy":"2023-03-28T04:15:49.869626Z","iopub.execute_input":"2023-03-28T04:15:49.869994Z","iopub.status.idle":"2023-03-28T04:16:00.838947Z","shell.execute_reply.started":"2023-03-28T04:15:49.869958Z","shell.execute_reply":"2023-03-28T04:16:00.837806Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Encoded Sentences Shape: torch.Size([14987, 113])\nEncoded NER Tags Shape: torch.Size([14987, 113])\n","output_type":"stream"}]},{"cell_type":"code","source":"# define the model architecture\nclass BiLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, num_layers, dropout):\n        super(BiLSTM, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(word_embed_matrix), freeze=False, padding_idx=0)\n        self.embedding_dropout = nn.Dropout(0.2)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=num_layers, bidirectional=True)\n        self.linear = nn.Linear(hidden_dim * 2, 128)\n        self.linear_dropout = nn.Dropout(0.2)\n        self.dropout = nn.Dropout(p = dropout)\n        self.activation = nn.ELU()\n        self.classifier = nn.Linear(128, output_size)\n        \n    def forward(self, x):\n        lstm_out = self.embedding(x) \n        lstm_out = self.embedding_dropout(lstm_out) \n        lstm_out, _ = self.lstm(lstm_out) \n        lstm_out = self.dropout(lstm_out)\n        lstm_out = self.linear(lstm_out)\n        lstm_out = self.linear_dropout(lstm_out)\n        lstm_out = self.activation(lstm_out)\n        lstm_out = self.classifier(lstm_out)\n        \n        return lstm_out","metadata":{"execution":{"iopub.status.busy":"2023-03-28T04:26:04.287444Z","iopub.execute_input":"2023-03-28T04:26:04.288176Z","iopub.status.idle":"2023-03-28T04:26:04.298906Z","shell.execute_reply.started":"2023-03-28T04:26:04.288136Z","shell.execute_reply":"2023-03-28T04:26:04.297704Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"# define the hyperparameters\nnum_epochs = 50\nbatch_size = 32\n\noutput_size = len(ner2idx)\nvocab_size = len(word2idx)\nembedding_dim = 101\nhidden_dim = 256\nnum_layers = 1\ndropout = 0.33\nlearning_rate = 0.5\nweight = [0.4 , 1, 1, 1.5, 1, 1.4, 1.5, 1, 1.2]","metadata":{"execution":{"iopub.status.busy":"2023-03-28T04:26:04.517570Z","iopub.execute_input":"2023-03-28T04:26:04.518348Z","iopub.status.idle":"2023-03-28T04:26:04.524241Z","shell.execute_reply.started":"2023-03-28T04:26:04.518307Z","shell.execute_reply":"2023-03-28T04:26:04.523084Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"# instantiate the model\nmodel = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_size, num_layers, dropout)\n\n# set up the optimizer and loss function\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-5)\nloss_fn = nn.CrossEntropyLoss(ignore_index=-1, weight= torch.tensor(weight)).cuda()\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate,\n                                                total_steps=num_epochs*(x.shape[0]//batch_size + 1),\n                                                anneal_strategy='linear')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.train()\n\n# loop through the data for the specified number of epochs\nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    num_batches = 0\n    \n    # shuffle the data for each epoch\n    indices = torch.randperm(len(x))\n    x = x[indices]\n    y = y[indices]\n    \n    # loop through the data in batches\n    for i in range(0, len(x), batch_size):\n        x_batch = x[i:i+batch_size]\n        y_batch = y[i:i+batch_size]\n        \n        # zero out the gradients\n        optimizer.zero_grad()\n        \n        # run the model on the current batch\n        outputs = model(x_batch.cuda())\n\n        # compute the loss and update the parameters\n        loss = loss_fn(outputs.view(-1, len(ner2idx)).cuda(), y_batch.view(-1).cuda())\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        epoch_loss += loss.item()\n        num_batches += 1\n\n    print('Epoch:', epoch+1 ,'Loss:', epoch_loss / num_batches)","metadata":{"id":"7katXS_KP8yC","outputId":"c32a15f8-fdc3-4534-ad33-5cc08ef389a1","execution":{"iopub.status.busy":"2023-03-28T04:26:04.920088Z","iopub.execute_input":"2023-03-28T04:26:04.920796Z","iopub.status.idle":"2023-03-28T04:33:25.011164Z","shell.execute_reply.started":"2023-03-28T04:26:04.920752Z","shell.execute_reply":"2023-03-28T04:33:25.010029Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"Epoch: 1 Loss: 0.6368294587966475\nEpoch: 2 Loss: 0.2861291265913418\nEpoch: 3 Loss: 0.22020957263103172\nEpoch: 4 Loss: 0.17746143159009756\nEpoch: 5 Loss: 0.1535319134132313\nEpoch: 6 Loss: 0.13279101976564825\nEpoch: 7 Loss: 0.11078787096607279\nEpoch: 8 Loss: 0.09852053387078649\nEpoch: 9 Loss: 0.08399453629125982\nEpoch: 10 Loss: 0.07613281777966568\nEpoch: 11 Loss: 0.06904096802823674\nEpoch: 12 Loss: 0.05686044940121297\nEpoch: 13 Loss: 0.053445063213676786\nEpoch: 14 Loss: 0.04569810239780845\nEpoch: 15 Loss: 0.04133638273577081\nEpoch: 16 Loss: 0.040145924974626095\nEpoch: 17 Loss: 0.03351839942220193\nEpoch: 18 Loss: 0.03047951704935173\nEpoch: 19 Loss: 0.026297738981385554\nEpoch: 20 Loss: 0.024992812153593356\nEpoch: 21 Loss: 0.021948009636550983\nEpoch: 22 Loss: 0.02131553989105812\nEpoch: 23 Loss: 0.01746241953896621\nEpoch: 24 Loss: 0.016528831303009448\nEpoch: 25 Loss: 0.015911366991932813\nEpoch: 26 Loss: 0.01393965713015553\nEpoch: 27 Loss: 0.014316197254321178\nEpoch: 28 Loss: 0.011673113206281826\nEpoch: 29 Loss: 0.011032497429518279\nEpoch: 30 Loss: 0.010479859331703888\nEpoch: 31 Loss: 0.0113707243740784\nEpoch: 32 Loss: 0.0106833925954305\nEpoch: 33 Loss: 0.009299455586852986\nEpoch: 34 Loss: 0.009445887874345978\nEpoch: 35 Loss: 0.009117178743917879\nEpoch: 36 Loss: 0.0077536056799414966\nEpoch: 37 Loss: 0.007618845555280248\nEpoch: 38 Loss: 0.007185252080305364\nEpoch: 39 Loss: 0.006848503009266983\nEpoch: 40 Loss: 0.007019240306838027\nEpoch: 41 Loss: 0.006470748555933017\nEpoch: 42 Loss: 0.006222099960397637\nEpoch: 43 Loss: 0.00527891738111958\nEpoch: 44 Loss: 0.005315529875739513\nEpoch: 45 Loss: 0.004977173797530197\nEpoch: 46 Loss: 0.005131266970333379\nEpoch: 47 Loss: 0.004167625417467827\nEpoch: 48 Loss: 0.004147621746748256\nEpoch: 49 Loss: 0.0037228260788824416\nEpoch: 50 Loss: 0.003840669436690667\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 'blstm2.pt'\ntorch.save(model.state_dict(), model_name)","metadata":{"id":"TVI0rde5vToE","execution":{"iopub.status.busy":"2023-03-28T04:33:25.013961Z","iopub.execute_input":"2023-03-28T04:33:25.014608Z","iopub.status.idle":"2023-03-28T04:33:25.038713Z","shell.execute_reply.started":"2023-03-28T04:33:25.014565Z","shell.execute_reply":"2023-03-28T04:33:25.037777Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# read the text file into a list of strings\nwith open('/data/dev', 'r') as f:\n    data = f.read().splitlines()\n\n# split each string into three columns and store in a list\nrows = []\nfor line in data:\n    cols = line.split()\n    if len(cols) == 3:\n        rows.append(cols)\n\n# create a pandas dataframe from the list of rows\ndf_dev = pd.DataFrame(rows, columns = [\"index\", \"token\", \"tag\"])\ndf_dev['index'] = df_dev['index'].astype(int)\n","metadata":{"id":"R-ptXntUP809","execution":{"iopub.status.busy":"2023-03-28T04:34:46.055574Z","iopub.execute_input":"2023-03-28T04:34:46.056640Z","iopub.status.idle":"2023-03-28T04:34:46.284113Z","shell.execute_reply.started":"2023-03-28T04:34:46.056585Z","shell.execute_reply":"2023-03-28T04:34:46.282823Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"model_2 = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_size, num_layers, dropout)\nmodel_2.load_state_dict(torch.load('/kaggle/working/' + model_name))\nmodel_2.eval()\n\nidx2ner = {v: k for k, v in ner2idx.items()}\n\n# create a list to store the predicted NER tags for each word\npredicted_tags = []\n\n# initialize the encoded sentence and NER tag lists\ndev_encoded_sentence = []\n\n# loop through each row in the dataframe\nfor i, row in df_dev.iterrows():\n    \n    # check if it is the beginning of a new sentence\n    if row['index'] == 1:\n        \n        # check if this is not the first sentence\n        if i > 0:\n            # encode the previous sentence and predict NER tags\n            dev_encoded_sentence = torch.LongTensor(dev_encoded_sentence)\n            with torch.no_grad():\n                output = model_2(dev_encoded_sentence)\n                predicted_tag_indices = output.argmax(dim=1)\n                predicted_tags.extend([idx2ner[idx.item()] for idx in predicted_tag_indices])\n            \n        # re-initialize the encoded sentence and NER tag lists\n        dev_encoded_sentence = []\n        \n    # if word is in all caps and if word has first letter as capital, consider the word in its as first letter capital form\n    if row['token'].isupper() and row['token'].title() in word2idx:\n        dev_encoded_sentence.append(word2idx[row['token'].title()])\n    \n    #if word exists in vocab, get the idx of the word \n    elif row['token'] in word2idx:\n        dev_encoded_sentence.append(word2idx[row['token']])\n    \n    #else assign index of unk\n    else:\n        dev_encoded_sentence.append(word2idx['unk'])\n    \n# encode the last sentence and predict NER tags\ndev_encoded_sentence = torch.LongTensor(dev_encoded_sentence)\n\nwith torch.no_grad():\n    output = model_2(dev_encoded_sentence)\n    predicted_tag_indices = output.argmax(dim=1)\n    predicted_tags.extend([idx2ner[idx.item()] for idx in predicted_tag_indices])\n    \n# add the predicted tags to the dataframe\ndf_dev['pred'] = predicted_tags\n","metadata":{"id":"mzOsWBhOtBCo","execution":{"iopub.status.busy":"2023-03-28T04:34:46.410933Z","iopub.execute_input":"2023-03-28T04:34:46.411536Z","iopub.status.idle":"2023-03-28T04:34:58.787803Z","shell.execute_reply.started":"2023-03-28T04:34:46.411499Z","shell.execute_reply":"2023-03-28T04:34:58.786750Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"with open('dev1_1.out', 'w') as f:\n    f_to_write = \"\"\n    first_ex = True\n    count = 1\n    for i_row, row in df_dev.iterrows():\n        if(row['index'] == 1):\n            if first_ex:\n                first_ex = False\n            else:\n                count = 1\n                f_to_write += \"\\n\"\n        f_to_write += str(count) + \" \" + row['token'] + \" \" + row['tag'] + \" \" + row['pred']  + \"\\n\"\n        count+=1\n    f.write(f_to_write)","metadata":{"id":"D85XegbOvtwn","execution":{"iopub.status.busy":"2023-03-28T04:34:58.789638Z","iopub.execute_input":"2023-03-28T04:34:58.789982Z","iopub.status.idle":"2023-03-28T04:35:01.914529Z","shell.execute_reply.started":"2023-03-28T04:34:58.789946Z","shell.execute_reply":"2023-03-28T04:35:01.913525Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"df_dev.head(30)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T04:24:26.083218Z","iopub.execute_input":"2023-03-28T04:24:26.083609Z","iopub.status.idle":"2023-03-28T04:24:26.096877Z","shell.execute_reply.started":"2023-03-28T04:24:26.083571Z","shell.execute_reply":"2023-03-28T04:24:26.095694Z"},"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"    index           token     tag   pred\n0       1         CRICKET       O      O\n1       2               -       O      O\n2       3  LEICESTERSHIRE   B-ORG  B-ORG\n3       4            TAKE       O      O\n4       5            OVER       O      O\n5       6              AT       O      O\n6       7             TOP       O      O\n7       8           AFTER       O      O\n8       9         INNINGS       O      O\n9      10         VICTORY       O      O\n10     11               .       O      O\n11      1          LONDON   B-LOC  B-LOC\n12      2      1996-08-30       O      O\n13      1            West  B-MISC  B-LOC\n14      2          Indian  I-MISC  B-LOC\n15      3     all-rounder       O  B-PER\n16      4            Phil   B-PER  B-PER\n17      5         Simmons   I-PER  I-PER\n18      6            took       O      O\n19      7            four       O      O\n20      8             for       O      O\n21      9              38       O      O\n22     10              on       O      O\n23     11          Friday       O      O\n24     12              as       O      O\n25     13  Leicestershire   B-ORG  B-ORG\n26     14            beat       O      O\n27     15        Somerset   B-ORG  B-ORG\n28     16              by       O      O\n29     17              an       O      O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>token</th>\n      <th>tag</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B-ORG</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>OVER</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>AT</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>TOP</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>AFTER</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>INNINGS</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>VICTORY</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>.</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>LONDON</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2</td>\n      <td>1996-08-30</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>West</td>\n      <td>B-MISC</td>\n      <td>B-LOC</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2</td>\n      <td>Indian</td>\n      <td>I-MISC</td>\n      <td>B-LOC</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3</td>\n      <td>all-rounder</td>\n      <td>O</td>\n      <td>B-PER</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>4</td>\n      <td>Phil</td>\n      <td>B-PER</td>\n      <td>B-PER</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>5</td>\n      <td>Simmons</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>6</td>\n      <td>took</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>7</td>\n      <td>four</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>8</td>\n      <td>for</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>9</td>\n      <td>38</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>10</td>\n      <td>on</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>11</td>\n      <td>Friday</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>12</td>\n      <td>as</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>13</td>\n      <td>Leicestershire</td>\n      <td>B-ORG</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>14</td>\n      <td>beat</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>15</td>\n      <td>Somerset</td>\n      <td>B-ORG</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>16</td>\n      <td>by</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>17</td>\n      <td>an</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}